{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Sum of Squares for error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t): \n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "t = np.zeros(5)\n",
    "t[1] = 1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 :  0.13170000000000004\n",
      "y2 :  0.53625\n"
     ]
    }
   ],
   "source": [
    "#1일 확률이 제일 높다고 추정\n",
    "y1 = [0.1,0.6,0.05,0.03,0.3]\n",
    "#4일 확률이 제일 높다고 추정\n",
    "y2 = [0.1,0.1,0.3,0.05,0.4]\n",
    "print(\"y1 : \",sum_squares_error(y1,t))\n",
    "print(\"y2 : \",sum_squares_error(y2,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 :  0.510825457099338\n",
      "y2 :  2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "#1일 확률이 제일 높다고 추정\n",
    "y1 = [0.1,0.6,0.05,0.03,0.3]\n",
    "#4일 확률이 제일 높다고 추정\n",
    "y2 = [0.1,0.1,0.3,0.05,0.4]\n",
    "print(\"y1 : \",cross_entropy(np.array(y1),np.array(t)))\n",
    "print(\"y2 : \",cross_entropy(np.array(y2),np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True,one_hot_label = True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7840"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 28*28 * 10(개)\n",
    "x_batch.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch용 cross entropy error\n",
    "# y = predict\n",
    "# t = label(0,1,2,3,4,...)\n",
    "def cross_entropy_error(y,t):\n",
    "    # 1차원 배열 -> 2차원 배열 ( [a,b,c] --> [[a,b,c]])\n",
    "    #형태를 통일\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) /  batch_size # t = 1 이므로 생략\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05 0.4 ]]\n",
      "1.9560103777150886\n"
     ]
    }
   ],
   "source": [
    "y = [[0.1,0.6,0.05,0.03,0.3],[0.1,0.1,0.3,0.05,0.4]]\n",
    "t = [[2,4]]\n",
    "y = np.array(y)\n",
    "t = np.array(t)\n",
    "print(y[np.arange(2),t])\n",
    "print(cross_entropy_error(y,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[6.11110793e-10 8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "#input : function / x(point)\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        tem_val = x[idx]\n",
    "        \n",
    "        # x +h\n",
    "        x[idx] =  tem_val + h\n",
    "        # f(x + h) 계산\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tem_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tem_val #값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient_V2(f,X):\n",
    "    h = 1e-4\n",
    "    if X.ndim == 1 :\n",
    "        return numerical_gradient(f,X)\n",
    "    \n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx , x in enumerate(X):\n",
    "            grad[idx] = numerical_gradient(f,x)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "#y = x1**2 = x2**2\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "print(numerical_gradient(function_2,np.array([3.0,4.0]))) #Not np.array([3,4]) --> 소수점을 다 버리고 int로 Return\n",
    "\n",
    "def gradient_descent(f,init_x,learning_rate = 0.01,step_num = 100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient_V2(f,x)\n",
    "        x = x - learning_rate*grad\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(gradient_descent(function_2,init_x = np.array([3.0,4.0]),learning_rate = 0.1,step_num = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53594649 -0.81226192  0.17115805]\n",
      " [-0.02069657 -0.75178745  0.43149146]]\n"
     ]
    }
   ],
   "source": [
    "class simpleNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #정규분포로 초기화( 크기는 (2,3))\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    # WX\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def soft_max(self,x):\n",
    "        c = np.max(x)\n",
    "        a = np.exp(x-c)\n",
    "        sum_a = np.sum(a)\n",
    "        return a / sum_a\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = self.soft_max(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30294099 -1.16396585  0.49103714]\n"
     ]
    }
   ],
   "source": [
    "# W의 size가 (2,3) 이므로 (1,2)로 사이즈 지정\n",
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "#랜덤 가중치에 의해 예측된 다음값\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#어느것이 가장 정답일까?\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.139931573926755"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정답데이터를 t로 입력하고 loss를 계산(예측 --> softmax --> cross entropy)\n",
    "t = np.array([0,0,1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy function\n",
    "def f(W):\n",
    "    return net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46156501 -0.42968843  0.89125344]\n",
      " [-0.69234752 -0.64453264  1.33688016]]\n"
     ]
    }
   ],
   "source": [
    "#기울기 계산해보기\n",
    "dW = numerical_gradient_V2(f,net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기울기 갱신하기(Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
