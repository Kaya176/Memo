{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', '가.'), ('Hi.', '안녕.'), ('Run!', '뛰어!'), ('Run.', '뛰어.'), ('Who?', '누구?')]\n"
     ]
    }
   ],
   "source": [
    "def open_file(file):\n",
    "    data = []\n",
    "    with open(file,encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            Real=tuple(line.strip().split(sep = \"\\t\")[:2])\n",
    "            data.append(Real)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "file = open_file(\"kor.txt\")\n",
    "print(file[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the sooner you do it , the better it is . <end>\n",
      "<start> 더 빨리할수록 좋다 . <end>\n"
     ]
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFC\",s)\n",
    "                   if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z가-힣ㄱ-ㅎㅏ-ㅣ?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "en_sentence = \"The sooner you do it, the better it is.\"\n",
    "kor_sentence = \"더 빨리할수록 좋다.\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(kor_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지 . 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면 , 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고 , 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면 , 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면 , 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야 . <end>\n"
     ]
    }
   ],
   "source": [
    "def create_datasets(num_examples):\n",
    "    word_pair = [[preprocess_sentence(s) for s in line] for line in file[:num_examples]]\n",
    "    return zip(*word_pair)\n",
    "\n",
    "en,kor = create_datasets(None)\n",
    "print(en[-1])\n",
    "print(kor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
    "    tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding = 'post')\n",
    "    \n",
    "    return tensor , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(num_examples = None):\n",
    "    tar_lang,inp_lang = create_datasets(None)\n",
    "    \n",
    "    input_tensor,input_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor,target_lang_tokenizer = tokenize(tar_lang)\n",
    "    return input_tensor,target_tensor, input_lang_tokenizer,target_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "num_examples = 1000\n",
    "input_tensor,target_tensor,input_lang_tokenizer,target_lang_tokenizer = load_dataset(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL\n",
    "input_examples = None\n",
    "input_tensor,target_tensor,input_lang_tokenizer,target_lang_tokenizer = load_dataset(num_examples)\n",
    "max_length_tar, max_length_inp = target_tensor.shape[1],input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_test_split\n",
    "input_tensor_train, input_tensor_val , target_tensor_train, target_tensor_val = train_test_split(input_tensor,target_tensor,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_tensor_train :  2910\n",
      "Input_tensor_val :  728\n",
      "Target_tensor_train :  2910\n",
      "Target_tensor_val :  728\n"
     ]
    }
   ],
   "source": [
    "#Show Length\n",
    "print(\"Input_tensor_train : \",len(input_tensor_train))\n",
    "print(\"Input_tensor_val : \",len(input_tensor_val))\n",
    "print(\"Target_tensor_train : \",len(target_tensor_train))\n",
    "print(\"Target_tensor_val : \",len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> <start>\n",
      "18 ----> 네가\n",
      "284 ----> 아는\n",
      "161 ----> 사람\n",
      "950 ----> 중에서\n",
      "111 ----> 누가\n",
      "44 ----> 가장\n",
      "1200 ----> 똑똑한\n",
      "155 ----> 사람이야\n",
      "4 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "1 ----> <start>\n",
      "73 ----> who\n",
      "14 ----> s\n",
      "11 ----> the\n",
      "184 ----> most\n",
      "916 ----> intelligent\n",
      "128 ----> person\n",
      "7 ----> you\n",
      "46 ----> know\n",
      "8 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang,tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "convert(input_lang_tokenizer,input_tensor_train[0])\n",
    "print()\n",
    "convert(target_lang_tokenizer,target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializer\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 97]), TensorShape([16, 112]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "#Architecture : Embedding -> GRU + Attention -> \n",
    "class Encoder(tf.keras.Model) :\n",
    "    def __init__(self,vocab_size,embedding_dim,kor_units,batch_sz):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.kor_units = kor_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.kor_units,\n",
    "                                      return_sequences = True,\n",
    "                                      return_state = True,\n",
    "                                      recurrent_initializer = \"glorot_uniform\")\n",
    "    def call(self,x,hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x,initial_state = hidden)\n",
    "        return output,state\n",
    "    \n",
    "    def initialize_hidden_state (self):\n",
    "        return tf.zeros((self.batch_sz,self.kor_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (16, 97, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size,embedding_dim,units,BATCH_SIZE)\n",
    "\n",
    "#sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output,sample_state = encoder(example_input_batch,sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,query,values):\n",
    "        query_with_time_axis = tf.expand_dims(query,1)\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis)+self.W2(values)))\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(score,axis = 1)\n",
    "        context_vector = attention_weight * values\n",
    "        context_vector = tf.reduce_sum(context_vector,axis = 1)\n",
    "        \n",
    "        return context_vector,attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (16, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (16, 97, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,dec_unit,batch_sz):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_unit\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                      return_sequences = True,\n",
    "                                      return_state = True,\n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "    \n",
    "    def call(self,x,hidden,kor_output):\n",
    "        context_vector,attention_weights = self.attention(hidden,kor_output)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x],axis = -1)\n",
    "        output,state = self.gru(x)\n",
    "        output = tf.reshape(output,(-1,output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x,state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 2481)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, kor_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        kor_output, kor_hidden = encoder(inp, kor_hidden)\n",
    "        \n",
    "        dec_hidden = kor_hidden\n",
    "        \n",
    "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, kor_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4798\n",
      "Epoch 1 Batch 100 Loss 0.3742\n",
      "Epoch 1 Loss 0.3420\n",
      "Time taken for 1 epoch 193.01 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3656\n",
      "Epoch 2 Batch 100 Loss 0.3089\n",
      "Epoch 2 Loss 0.2745\n",
      "Time taken for 1 epoch 82.25 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2774\n",
      "Epoch 3 Batch 100 Loss 0.2423\n",
      "Epoch 3 Loss 0.2422\n",
      "Time taken for 1 epoch 81.43 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2280\n",
      "Epoch 4 Batch 100 Loss 0.1818\n",
      "Epoch 4 Loss 0.2113\n",
      "Time taken for 1 epoch 82.94 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2015\n",
      "Epoch 5 Batch 100 Loss 0.2330\n",
      "Epoch 5 Loss 0.1834\n",
      "Time taken for 1 epoch 81.87 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1450\n",
      "Epoch 6 Batch 100 Loss 0.1143\n",
      "Epoch 6 Loss 0.1559\n",
      "Time taken for 1 epoch 81.38 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1678\n",
      "Epoch 7 Batch 100 Loss 0.1113\n",
      "Epoch 7 Loss 0.1287\n",
      "Time taken for 1 epoch 80.73 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1058\n",
      "Epoch 8 Batch 100 Loss 0.0972\n",
      "Epoch 8 Loss 0.1042\n",
      "Time taken for 1 epoch 81.29 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0690\n",
      "Epoch 9 Batch 100 Loss 0.1184\n",
      "Epoch 9 Loss 0.0813\n",
      "Time taken for 1 epoch 80.81 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0518\n",
      "Epoch 10 Batch 100 Loss 0.0518\n",
      "Epoch 10 Loss 0.0621\n",
      "Time taken for 1 epoch 81.31 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0382\n",
      "Epoch 11 Batch 100 Loss 0.0387\n",
      "Epoch 11 Loss 0.0481\n",
      "Time taken for 1 epoch 80.74 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0335\n",
      "Epoch 12 Batch 100 Loss 0.0379\n",
      "Epoch 12 Loss 0.0370\n",
      "Time taken for 1 epoch 83.50 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0245\n",
      "Epoch 13 Batch 100 Loss 0.0209\n",
      "Epoch 13 Loss 0.0281\n",
      "Time taken for 1 epoch 81.69 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0138\n",
      "Epoch 14 Batch 100 Loss 0.0181\n",
      "Epoch 14 Loss 0.0204\n",
      "Time taken for 1 epoch 83.96 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0101\n",
      "Epoch 15 Batch 100 Loss 0.0135\n",
      "Epoch 15 Loss 0.0147\n",
      "Time taken for 1 epoch 82.45 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0109\n",
      "Epoch 16 Batch 100 Loss 0.0154\n",
      "Epoch 16 Loss 0.0115\n",
      "Time taken for 1 epoch 82.55 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0094\n",
      "Epoch 17 Batch 100 Loss 0.0068\n",
      "Epoch 17 Loss 0.0095\n",
      "Time taken for 1 epoch 83.02 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0030\n",
      "Epoch 18 Batch 100 Loss 0.0055\n",
      "Epoch 18 Loss 0.0076\n",
      "Time taken for 1 epoch 82.76 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0024\n",
      "Epoch 19 Batch 100 Loss 0.0025\n",
      "Epoch 19 Loss 0.0060\n",
      "Time taken for 1 epoch 81.98 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0014\n",
      "Epoch 20 Batch 100 Loss 0.0037\n",
      "Epoch 20 Loss 0.0055\n",
      "Time taken for 1 epoch 82.29 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0036\n",
      "Epoch 21 Batch 100 Loss 0.0043\n",
      "Epoch 21 Loss 0.0055\n",
      "Time taken for 1 epoch 84.16 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0032\n",
      "Epoch 22 Batch 100 Loss 0.0044\n",
      "Epoch 22 Loss 0.0057\n",
      "Time taken for 1 epoch 83.28 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0071\n",
      "Epoch 23 Batch 100 Loss 0.0028\n",
      "Epoch 23 Loss 0.0074\n",
      "Time taken for 1 epoch 83.33 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0066\n",
      "Epoch 24 Batch 100 Loss 0.0026\n",
      "Epoch 24 Loss 0.0078\n",
      "Time taken for 1 epoch 83.18 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0025\n",
      "Epoch 25 Batch 100 Loss 0.0031\n",
      "Epoch 25 Loss 0.0064\n",
      "Time taken for 1 epoch 84.03 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0036\n",
      "Epoch 26 Batch 100 Loss 0.0015\n",
      "Epoch 26 Loss 0.0050\n",
      "Time taken for 1 epoch 84.43 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0037\n",
      "Epoch 27 Batch 100 Loss 0.0061\n",
      "Epoch 27 Loss 0.0048\n",
      "Time taken for 1 epoch 82.05 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0014\n",
      "Epoch 28 Batch 100 Loss 0.0060\n",
      "Epoch 28 Loss 0.0054\n",
      "Time taken for 1 epoch 82.56 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0081\n",
      "Epoch 29 Batch 100 Loss 0.0062\n",
      "Epoch 29 Loss 0.0046\n",
      "Time taken for 1 epoch 81.72 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0021\n",
      "Epoch 30 Batch 100 Loss 0.0009\n",
      "Epoch 30 Loss 0.0039\n",
      "Time taken for 1 epoch 82.00 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    kor_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, kor_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "def evaluate(sentence) :\n",
    "    attention_plot = np.zeros((max_length_tar,max_length_inp))\n",
    "    \n",
    "    #preprocessing\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen = max_length_inp,padding = 'post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1,units))]\n",
    "    kor_output,kor_hidden = encoder(inputs,hidden)\n",
    "    \n",
    "    dec_hidden = kor_hidden\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']],0)\n",
    "    \n",
    "    for t in range(max_length_tar):\n",
    "        predication,dec_hidden,attention_weight = decoder(dec_input,dec_hidden,kor_output)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weight,(-1,))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predication[0]).numpy()\n",
    "        \n",
    "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        \n",
    "        if target_lang_tokenizer.index_word[predicted_id] =='<end>':\n",
    "            return result,sentence,attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id],0)\n",
    "    return result,sentence,attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention,sentence,predicated_sentence):\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention,cmap = 'viridis')\n",
    "    \n",
    "    fontdict = {'fontsize' : 14}\n",
    "    \n",
    "    ax.set_xticklabels(['']+sentence,fontdict = fontdict,rotation = 90)\n",
    "    ax.set_yticklabels(['']+predicated_sentence,fontdict = fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result,sentence,attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print(f\"Input : {sentence}\")\n",
    "    print(f\"Predicted translation : {result}\")\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')),:len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot,sentence.split(' '),result.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1f39b654ec8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : <start> 리눅스는 오픈 소스 소프트웨어야 . <end>\n",
      "Predicted translation : linux is open source . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 47532 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 45573 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 54536 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 49548 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 54532 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 53944 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 50920 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 50556 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 47532 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 45573 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 54536 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 49548 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 54532 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 53944 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 50920 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\ing_lab\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 50556 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('리눅스는 오픈 소스 소프트웨어야.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
